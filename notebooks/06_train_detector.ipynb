{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "29cbafc7",
=======
   "execution_count": 3,
   "id": "91df69c6",
>>>>>>> 2630bc3 (classification w/ working test)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Using device: mps\n",
      "Classes: {1: 'dent', 2: 'scratch', 3: 'crack', 4: 'glass shatter', 5: 'lamp broken', 6: 'tire flat'}\n"
=======
      "/Users/prisriva/Desktop/fai_final/data/raw/train/images\n",
      "/Users/prisriva/Desktop/fai_final/data/raw/train/annotations.json\n"
>>>>>>> 2630bc3 (classification w/ working test)
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#Imports, paths to (augmented) detector dataset, seeds, hyperparameters.\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Paths (aligned with augmentation notebook)\n",
    "BASE_DATA_ROOT = Path(\"/Users/stephenmacris/Documents/School/CS5100/Project/CarDD_release/CarDD_COCO\")\n",
    "\n",
    "DETECTOR_ROOT = BASE_DATA_ROOT / \"detector\"\n",
    "AUG_ROOT = BASE_DATA_ROOT / \"augmented\" / \"detector\"\n",
    "\n",
    "TRAIN_IMG_DIR = DETECTOR_ROOT / \"train\" / \"images\"\n",
    "TRAIN_ANN = DETECTOR_ROOT / \"train\" / \"annotations\" / \"annotations.json\"\n",
    "\n",
    "VAL_IMG_DIR = DETECTOR_ROOT / \"val\" / \"images\"\n",
    "VAL_ANN = DETECTOR_ROOT / \"val\" / \"annotations\" / \"annotations.json\"\n",
    "\n",
    "TEST_IMG_DIR = DETECTOR_ROOT / \"test\" / \"images\"\n",
    "TEST_ANN = DETECTOR_ROOT / \"test\" / \"annotations\" / \"annotations.json\"\n",
    "\n",
    "with open(TRAIN_ANN, \"r\") as f:\n",
    "    train_coco = json.load(f)\n",
    "\n",
    "categories = train_coco.get(\"categories\", [])\n",
    "category_id_to_name = {c[\"id\"]: c[\"name\"] for c in categories}\n",
    "NUM_CLASSES = len(categories) + 1  # +1 for background\n",
    "print(\"Classes:\", category_id_to_name)\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CHECKPOINT_PATH = \"fasterrcnn_cardd_best.pt\"\n"
=======
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# paths\n",
    "\n",
    "root_dir   = \"/Users/prisriva/Desktop/fai_final\"\n",
    "data_root  = os.path.join(root_dir, \"data\", \"raw\")\n",
    "\n",
    "train_dir  = os.path.join(data_root, \"train\")\n",
    "val_dir    = os.path.join(data_root, \"val\")\n",
    "test_dir   = os.path.join(data_root, \"test\")\n",
    "\n",
    "train_img_dir = os.path.join(train_dir, \"images\")\n",
    "val_img_dir   = os.path.join(val_dir, \"images\")\n",
    "test_img_dir  = os.path.join(test_dir, \"images\")\n",
    "\n",
    "train_ann = os.path.join(train_dir, \"annotations.json\")\n",
    "val_ann   = os.path.join(val_dir, \"annotations.json\")\n",
    "test_ann  = os.path.join(test_dir, \"annotations.json\")\n",
    "\n",
    "print(train_img_dir)\n",
    "print(train_ann)\n"
>>>>>>> 2630bc3 (classification w/ working test)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
   "id": "bd3496e9",
=======
   "id": "04980525",
>>>>>>> 2630bc3 (classification w/ working test)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Train batches: 704, Val batches: 203\n"
=======
      "Loaded:\n",
      "  TRAIN: 2816 images, 6211 annos\n",
      "  VAL:   810 images, 1744 annos\n",
      "  TEST:  374 images, 785 annos\n",
      "\n",
      "TRAIN class distribution:\n",
      "  Class 1: 1806\n",
      "  Class 2: 2560\n",
      "  Class 3: 651\n",
      "  Class 4: 475\n",
      "  Class 5: 494\n",
      "  Class 6: 225\n",
      "\n",
      "Example image_id -> classes (train):\n",
      "1 : [2, 6]\n",
      "2 : [6]\n",
      "3 : [6]\n",
      "4 : [6]\n",
      "5 : [6]\n"
>>>>>>> 2630bc3 (classification w/ working test)
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#Dataset/Dataloader: custom Dataset to read images + bboxes/labels from COCO, apply train/val transforms, \n",
    "#collate function for variable targets.\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        return F.to_tensor(image), target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.p:\n",
    "            width, _ = image.size\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            if \"boxes\" in target:\n",
    "                boxes = target[\"boxes\"].clone()\n",
    "                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
    "                target[\"boxes\"] = boxes\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train=True):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)\n",
    "\n",
    "class CocoDetectionDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, ann_path: Path, transforms=None):\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            coco = json.load(f)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transforms = transforms\n",
    "        self.id_to_image = {img[\"id\"]: img for img in coco.get(\"images\", [])}\n",
    "        self.anns_by_image = defaultdict(list)\n",
    "        for ann in coco.get(\"annotations\", []):\n",
    "            self.anns_by_image[ann[\"image_id\"]].append(ann)\n",
    "        self.image_ids = list(self.id_to_image.keys())\n",
=======
    "def load_coco_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_data = load_coco_json(train_ann)\n",
    "val_data   = load_coco_json(val_ann)\n",
    "test_data  = load_coco_json(test_ann)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(f\"  TRAIN: {len(train_data['images'])} images, {len(train_data['annotations'])} annos\")\n",
    "print(f\"  VAL:   {len(val_data['images'])} images, {len(val_data['annotations'])} annos\")\n",
    "print(f\"  TEST:  {len(test_data['images'])} images, {len(test_data['annotations'])} annos\")\n",
    "\n",
    "def count_class_freq(coco):\n",
    "    counts = defaultdict(int)\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        counts[ann[\"category_id\"]] += 1\n",
    "    return counts\n",
    "\n",
    "train_class_counts = count_class_freq(train_data)\n",
    "print(\"\\nTRAIN class distribution:\")\n",
    "for cid in sorted(train_class_counts):\n",
    "    print(f\"  Class {cid}: {train_class_counts[cid]}\")\n",
    "\n",
    "def build_image_to_classes(coco):\n",
    "    mapping = defaultdict(set)\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        mapping[ann[\"image_id\"]].add(ann[\"category_id\"])\n",
    "    return {k: sorted(list(v)) for k, v in mapping.items()}\n",
    "\n",
    "train_image_to_classes = build_image_to_classes(train_data)\n",
    "val_image_to_classes   = build_image_to_classes(val_data)\n",
    "\n",
    "print(\"\\nExample image_id -> classes (train):\")\n",
    "for k in list(train_image_to_classes.keys())[:5]:\n",
    "    print(k, \":\", train_image_to_classes[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66b5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prisriva/miniconda3/envs/cardd/lib/python3.10/site-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "# Albumentations transforms for detection\n",
    "\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.Rotate(limit=10, border_mode=0, p=0.5),\n",
    "\n",
    "        # ensure float32 + [0,1] before tensor conversion\n",
    "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"coco\",          # [x, y, w, h]\n",
    "        min_area=1,\n",
    "        min_visibility=0.0,     # don't drop boxes based on visibility\n",
    "        label_fields=[\"labels\"],\n",
    "        check_each_transform=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"coco\",\n",
    "        min_area=1,\n",
    "        min_visibility=0.0,\n",
    "        label_fields=[\"labels\"],\n",
    "        check_each_transform=False,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba9dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDDDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    COCO-style detection dataset for Faster R-CNN using Albumentations.\n",
    "    Returns:\n",
    "        image_tensor (FloatTensor[C,H,W] in [0,1])\n",
    "        target dict: boxes (xyxy), labels, area, iscrowd, image_id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, coco_dict, images_dir, transform=None):\n",
    "        self.coco = coco_dict\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = self.coco[\"images\"]\n",
    "        self.annotations = self.coco[\"annotations\"]\n",
    "\n",
    "        # image_id -> image info\n",
    "        self.image_id_to_info = {img[\"id\"]: img for img in self.images}\n",
    "\n",
    "        # image_id -> list[annotations]\n",
    "        self.anns_by_image = defaultdict(list)\n",
    "        for ann in self.annotations:\n",
    "            self.anns_by_image[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "        self.image_ids = sorted(self.image_id_to_info.keys())\n",
>>>>>>> 2630bc3 (classification w/ working test)
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
<<<<<<< HEAD
    "        img_info = self.id_to_image[image_id]\n",
    "        img_path = self.images_dir / img_info[\"file_name\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for ann in self.anns_by_image.get(image_id, []):\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "            areas.append(ann.get(\"area\", w * h))\n",
    "            iscrowd.append(ann.get(\"iscrowd\", 0))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        areas = torch.tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": iscrowd,\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = CocoDetectionDataset(TRAIN_IMG_DIR, TRAIN_ANN, transforms=get_transform(train=True))\n",
    "val_dataset = CocoDetectionDataset(VAL_IMG_DIR, VAL_ANN, transforms=get_transform(train=False))\n",
    "\n",
    "test_dataset = None\n",
    "if TEST_ANN.exists():\n",
    "    test_dataset = CocoDetectionDataset(TEST_IMG_DIR, TEST_ANN, transforms=get_transform(train=False))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "test_loader = None\n",
    "if test_dataset:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
=======
    "        info = self.image_id_to_info[image_id]\n",
    "        filename = info[\"file_name\"]\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        anns = self.anns_by_image[image_id]\n",
    "\n",
    "        coco_boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]  # COCO format\n",
    "            coco_boxes.append([x, y, w, h])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Albumentations transform\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(\n",
    "                image=img_np,\n",
    "                bboxes=coco_boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            img_tensor = transformed[\"image\"]             # float32 CHW [0,1]\n",
    "            coco_boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"labels\"]\n",
    "        else:\n",
    "            img_tensor = T.ToTensor()(img)                # float32 CHW [0,1]\n",
    "\n",
    "        # Convert coco â†’ xyxy + recompute areas/iscrowd\n",
    "        if len(coco_boxes) > 0:\n",
    "            xyxy_boxes = []\n",
    "            areas = []\n",
    "            iscrowd = []\n",
    "\n",
    "            for (x, y, w, h) in coco_boxes:\n",
    "                xyxy_boxes.append([x, y, x + w, y + h])\n",
    "                areas.append(w * h)\n",
    "                iscrowd.append(0)\n",
    "\n",
    "            boxes = torch.tensor(xyxy_boxes, dtype=torch.float32)\n",
    "            labels_t = torch.tensor(labels, dtype=torch.int64)\n",
    "            areas_t = torch.tensor(areas, dtype=torch.float32)\n",
    "            iscrowd_t = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_t = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas_t = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd_t = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels_t,\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "            \"area\": areas_t,\n",
    "            \"iscrowd\": iscrowd_t,\n",
    "        }\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def detection_collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    return list(images), list(targets)\n"
>>>>>>> 2630bc3 (classification w/ working test)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "c9379e60",
=======
   "execution_count": 7,
   "id": "c749d891",
>>>>>>> 2630bc3 (classification w/ working test)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Using pretrained MobileNetV3-320 FPN weights; replacing head for NUM_CLASSES\n",
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(320,), max_size=480, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
      "            (2): Hardswish()\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Hardsigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv2dNormActivation(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-1): 2 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
=======
      "Train images: 2816\n",
      "Val images:   810\n",
      "Test images:  374\n",
      "\n",
      "Images per class (for sampling):\n",
      "  Class 1: 1242\n",
      "  Class 2: 1507\n",
      "  Class 3: 434\n",
      "  Class 4: 469\n",
      "  Class 5: 489\n",
      "  Class 6: 219\n",
      "\n",
      "Batch size: 4\n",
      "Image tensor shape: torch.Size([3, 750, 1000])\n",
      "Target keys: dict_keys(['boxes', 'labels', 'image_id', 'area', 'iscrowd'])\n"
>>>>>>> 2630bc3 (classification w/ working test)
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#Model definition: choose detector (e.g., torchvision fasterrcnn_resnet50_fpn or YOLOv5/YOLOv8 if available); \n",
    "#adapt num_classes to the damage categories.\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "    FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "try:\n",
    "    weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "    print(\"Using pretrained MobileNetV3-320 FPN weights; replacing head for NUM_CLASSES\")\n",
    "except Exception:\n",
    "    weights = None\n",
    "    print(\"Pretrained weights unavailable; initializing backbone randomly and replacing head\")\n",
    "\n",
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(\n",
    "    weights=weights,\n",
    "    min_size=320,   # shorter side\n",
    "    max_size=480,   # cap the longer side\n",
    ")\n",
    "\n",
    "# replace the head for your classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "\n",
    "model.to(DEVICE)\n",
    "print(model)\n"
=======
    "# Datasets\n",
    "\n",
    "train_dataset = CarDDDetectionDataset(\n",
    "    coco_dict=train_data,\n",
    "    images_dir=train_img_dir,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "val_dataset = CarDDDetectionDataset(\n",
    "    coco_dict=val_data,\n",
    "    images_dir=val_img_dir,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = CarDDDetectionDataset(\n",
    "    coco_dict=test_data,\n",
    "    images_dir=test_img_dir,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "print(\"Train images:\", len(train_dataset))\n",
    "print(\"Val images:  \", len(val_dataset))\n",
    "print(\"Test images: \", len(test_dataset))\n",
    "\n",
    "\n",
    "# Stratified sampling weights (image-level)\n",
    "\n",
    "class_image_counts = defaultdict(int)\n",
    "for img_id, classes in train_image_to_classes.items():\n",
    "    for c in classes:\n",
    "        class_image_counts[c] += 1\n",
    "\n",
    "print(\"\\nImages per class (for sampling):\")\n",
    "for cid in sorted(class_image_counts):\n",
    "    print(f\"  Class {cid}: {class_image_counts[cid]}\")\n",
    "\n",
    "weights = []\n",
    "for idx, img_id in enumerate(train_dataset.image_ids):\n",
    "    classes = train_image_to_classes.get(img_id, [])\n",
    "    if not classes:\n",
    "        w = 1.0\n",
    "    else:\n",
    "        w = sum(1.0 / class_image_counts[c] for c in classes)\n",
    "    weights.append(w)\n",
    "\n",
    "weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(weights),\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=detection_collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=detection_collate_fn,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=detection_collate_fn,\n",
    ")\n",
    "\n",
    "batch_images, batch_targets = next(iter(train_loader))\n",
    "print(\"\\nBatch size:\", len(batch_images))\n",
    "print(\"Image tensor shape:\", batch_images[0].shape)\n",
    "print(\"Target keys:\", batch_targets[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e190650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "NUM_CLASSES = 7  # background + 6 damage types\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=\"DEFAULT\"\n",
    ")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
>>>>>>> 2630bc3 (classification w/ working test)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "418c1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 1/704 [00:44<8:35:40, 44.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 0/704 | loss 3.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1:   0%|          | 3/704 [06:29<23:28:15, 120.54s/batch]"
     ]
    }
   ],
   "source": [
    "#Training loop: optimizer/scheduler setup, epoch loop with loss logging, checkpointing best model by val mAP.\n",
    "from torchvision.ops import box_iou\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_map(model, data_loader, device, iou_thresholds=None, score_thresh=0.05):\n",
    "    if iou_thresholds is None:\n",
    "        iou_thresholds = [0.5] + [round(x, 2) for x in np.arange(0.55, 0.96, 0.05)]\n",
    "\n",
    "    stats_per_cls = {t: defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0}) for t in iou_thresholds}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                gt_boxes = target[\"boxes\"].to(device)\n",
    "                gt_labels = target[\"labels\"].to(device)\n",
    "\n",
    "                pred_boxes = output[\"boxes\"].to(device)\n",
    "                pred_labels = output[\"labels\"].to(device)\n",
    "                scores = output[\"scores\"].to(device)\n",
    "\n",
    "                keep = scores >= score_thresh\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_labels = pred_labels[keep]\n",
    "\n",
    "                for t in iou_thresholds:\n",
    "                    matched = set()\n",
    "                    for pb, pl in zip(pred_boxes, pred_labels):\n",
    "                        cls = int(pl.item())\n",
    "                        mask = (gt_labels == pl)\n",
    "                        if mask.sum() == 0:\n",
    "                            stats_per_cls[t][cls][\"fp\"] += 1\n",
    "                            continue\n",
    "\n",
    "                        ious = box_iou(pb.unsqueeze(0), gt_boxes[mask]).squeeze(0)\n",
    "                        if ious.numel() == 0:\n",
    "                            stats_per_cls[t][cls][\"fp\"] += 1\n",
    "                            continue\n",
    "                        max_iou, max_idx = ious.max(0)\n",
    "                        if max_iou >= t:\n",
    "                            global_idx = mask.nonzero(as_tuple=False).squeeze(1)[max_idx].item()\n",
    "                            if global_idx not in matched:\n",
    "                                matched.add(global_idx)\n",
    "                                stats_per_cls[t][cls][\"tp\"] += 1\n",
    "                            else:\n",
    "                                stats_per_cls[t][cls][\"fp\"] += 1\n",
    "                        else:\n",
    "                            stats_per_cls[t][cls][\"fp\"] += 1\n",
    "\n",
    "                    # FN: ground truths of each class that were not matched\n",
    "                    for cls in gt_labels.unique():\n",
    "                        cls_id = int(cls.item())\n",
    "                        cls_mask = (gt_labels == cls)\n",
    "                        gt_indices = cls_mask.nonzero(as_tuple=False).squeeze(1).tolist()\n",
    "                        matched_cls = [idx for idx in matched if int(gt_labels[idx].item()) == cls_id]\n",
    "                        fn = len(gt_indices) - len(matched_cls)\n",
    "                        stats_per_cls[t][cls_id][\"fn\"] += max(fn, 0)\n",
    "\n",
    "    map_per_t = []\n",
    "    map50 = 0.0\n",
    "    per_class_map50 = {}\n",
    "\n",
    "    for idx, t in enumerate(iou_thresholds):\n",
    "        cls_scores = []\n",
    "        for cls_id, vals in stats_per_cls[t].items():\n",
    "            tp, fp, fn = vals[\"tp\"], vals[\"fp\"], vals[\"fn\"]\n",
    "            denom = tp + fp + fn + 1e-6\n",
    "            cls_score = tp / denom\n",
    "            cls_scores.append(cls_score)\n",
    "            if t == 0.5:\n",
    "                per_class_map50[cls_id] = cls_score\n",
    "        if cls_scores:\n",
    "            score_t = float(np.mean(cls_scores))\n",
    "            map_per_t.append(score_t)\n",
    "            if t == 0.5:\n",
    "                map50 = score_t\n",
    "        else:\n",
    "            map_per_t.append(0.0)\n",
    "            if t == 0.5:\n",
    "                map50 = 0.0\n",
    "\n",
    "    map5095 = float(np.mean(map_per_t)) if map_per_t else 0.0\n",
    "    return map50, map5095, per_class_map50\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "best_map50 = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, (images, targets) in enumerate(tqdm(train_loader, desc=f\"train epoch {epoch+1}\", unit=\"batch\")):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [\n",
    "            {k: v.to(DEVICE) if torch.is_tensor(v) else v for k, v in t.items()}\n",
    "            for t in targets\n",
    "        ]\n",
=======
   "execution_count": 9,
   "id": "4b75e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_every=20):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(data_loader):\n",
    "\n",
    "        # One-time debug for dtype/range:\n",
    "        if epoch == 1 and step == 0:\n",
    "            print(\"\\n=== DEBUG IMAGE CHECK (TRAIN) ===\")\n",
    "            print(\"dtype:\", images[0].dtype)\n",
    "            print(\"shape:\", images[0].shape)\n",
    "            print(\"min/max:\", images[0].min().item(), images[0].max().item())\n",
    "            print(\"================================\\n\")\n",
    "\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
>>>>>>> 2630bc3 (classification w/ working test)
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
<<<<<<< HEAD
    "        epoch_loss += losses.item()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  batch {i}/{len(train_loader)} | loss {losses.item():.4f}\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    avg_loss = epoch_loss / max(len(train_loader), 1)\n",
    "\n",
    "    val_map50, val_map5095, _ = evaluate_map(model, val_loader, DEVICE)\n",
    "\n",
    "    if val_map50 > best_map50:\n",
    "        best_map50 = val_map50\n",
    "        torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
    "        print(f\"Epoch {epoch+1}: new best mAP@0.5={val_map50:.4f} â†’ saved {CHECKPOINT_PATH}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | loss={avg_loss:.4f} | val mAP@0.5={val_map50:.4f} | val mAP@[0.5:0.95]={val_map5095:.4f}\"\n",
    "    )\n"
=======
    "        loss_value = losses.item()\n",
    "        running_loss += loss_value\n",
    "\n",
    "        if (step + 1) % print_every == 0:\n",
    "            avg = running_loss / (step + 1)\n",
    "            print(f\"[Epoch {epoch} | Step {step+1}/{len(data_loader)}] \"\n",
    "                  f\"Loss: {loss_value:.4f} (avg {avg:.4f})\")\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(data_loader))\n",
    "    print(f\"===> Epoch {epoch} TRAIN avg loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeaad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=EPOCH 1/10 \n",
      "\n",
      "=== DEBUG IMAGE CHECK (TRAIN) ===\n",
      "dtype: torch.float32\n",
      "shape: torch.Size([3, 750, 1000])\n",
      "min/max: 0.0 1.0\n",
      "================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "os.makedirs(os.path.join(root_dir, \"weights\"), exist_ok=True)\n",
    "ckpt_path = os.path.join(root_dir, \"weights\", \"fasterrcnn_cardd_best.pth\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=EPOCH {epoch}/{NUM_EPOCHS} \")\n",
    "    start_t = time()\n",
    "\n",
    "    # TRAIN\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "\n",
    "    # VALIDATION (loss + accuracy + F1)\n",
    "    val_loss, val_acc, val_f1, val_prec, val_rec = compute_detection_metrics(\n",
    "        model, val_loader, device\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    duration = time() - start_t\n",
    "    print(f\"[Epoch {epoch}] Duration: {duration:.2f}s\")\n",
    "    print(f\"[Epoch {epoch}] TRAIN loss: {train_loss:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] VAL   loss: {val_loss:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] VAL   acc:  {val_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] VAL   F1:   {val_f1:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] VAL   precision: {val_prec:.4f} | recall: {val_rec:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"[INFO] Saved new best model â†’ {ckpt_path}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Best val loss:\", best_val_loss)\n",
    "\n",
    "\n",
    "# FINAL TEST EVALUATION\n",
    "\n",
    "print(\"\\nFINAL TEST EVALUATION\")\n",
    "\n",
    "test_loss, test_acc, test_f1, test_prec, test_rec = compute_detection_metrics(\n",
    "    model, test_loader, device\n",
    ")\n",
    "\n",
    "print(f\"[TEST] Loss:      {test_loss:.4f}\")\n",
    "print(f\"[TEST] Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"[TEST] F1 Score:  {test_f1:.4f}\")\n",
    "print(f\"[TEST] Precision: {test_prec:.4f}\")\n",
    "print(f\"[TEST] Recall:    {test_rec:.4f}\")\n"
>>>>>>> 2630bc3 (classification w/ working test)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "80eaef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation: compute mAP@0.5 and mAP@[0.5:0.95] on val/test; print per-class AP.\n",
    "# Load best checkpoint if available\n",
    "if Path(CHECKPOINT_PATH).exists():\n",
    "    state = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(state)\n",
    "    print(f\"Loaded checkpoint: {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(\"No checkpoint found; evaluating current model state.\")\n",
    "\n",
    "val_map50, val_map5095, val_per_class = evaluate_map(model, val_loader, DEVICE)\n",
    "print(f\"Validation mAP@0.5: {val_map50:.4f} | mAP@[0.5:0.95]: {val_map5095:.4f}\")\n",
    "for cls_id, score in sorted(val_per_class.items()):\n",
    "    name = category_id_to_name.get(cls_id, str(cls_id))\n",
    "    print(f\"  {name}: {score:.4f}\")\n",
    "\n",
    "if test_loader is not None:\n",
    "    test_map50, test_map5095, test_per_class = evaluate_map(model, test_loader, DEVICE)\n",
    "    print(f\"\\nTest mAP@0.5: {test_map50:.4f} | mAP@[0.5:0.95]: {test_map5095:.4f}\")\n",
    "    for cls_id, score in sorted(test_per_class.items()):\n",
    "        name = category_id_to_name.get(cls_id, str(cls_id))\n",
    "        print(f\"  {name}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"Test set not found; skipping test evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e041f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference demo: run trained detector on a few test images, visualize and save predicted boxes/scores for qualitative review\n",
    "from PIL import ImageDraw\n",
    "\n",
    "sample_images = list(TEST_IMG_DIR.glob(\"*.jpg\"))[:3] if TEST_IMG_DIR.exists() else []\n",
    "if not sample_images:\n",
    "    sample_images = list(VAL_IMG_DIR.glob(\"*.jpg\"))[:3]\n",
    "\n",
    "score_threshold = 0.5\n",
    "model.eval()\n",
    "\n",
    "for img_path in sample_images:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model([img_tensor])[0]\n",
    "\n",
    "    keep = output[\"scores\"] >= score_threshold\n",
    "    boxes = output[\"boxes\"][keep].cpu()\n",
    "    labels = output[\"labels\"][keep].cpu()\n",
    "    scores = output[\"scores\"][keep].cpu()\n",
    "\n",
    "    vis = img.copy()\n",
    "    draw = ImageDraw.Draw(vis)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        cls_name = category_id_to_name.get(int(label.item()), str(int(label.item())))\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "        draw.text((x1 + 2, y1 + 2), f\"{cls_name}: {score:.2f}\", fill=\"yellow\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(img_path.name)\n",
    "    plt.imshow(vis)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
=======
   "id": "9a64b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model (after training)\n",
    "final_path = os.path.join(root_dir, \"weights\", \"fasterrcnn_cardd_final.pth\")\n",
    "torch.save(model.state_dict(), final_path)\n",
    "print(f\"[INFO] Final model saved â†’ {final_path}\")\n"
>>>>>>> 2630bc3 (classification w/ working test)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
   "display_name": "Python (cardd)",
   "language": "python",
   "name": "cardd"
>>>>>>> 2630bc3 (classification w/ working test)
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.6"
=======
   "version": "3.10.19"
>>>>>>> 2630bc3 (classification w/ working test)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
